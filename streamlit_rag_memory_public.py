import os
import streamlit as st
import tempfile

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    split_docs = text_splitter.split_documents(_docs)
    persist_directory = "./chroma_db"
    vectorstore = Chroma.from_documents(
        split_docs, 
        OpenAIEmbeddings(model='text-embedding-3-small'),
        persist_directory=persist_directory
    )
    return vectorstore

@st.cache_resource
def get_vectorstore(_docs):
    persist_directory = "./chroma_db"
    if os.path.exists(persist_directory):
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=OpenAIEmbeddings(model='text-embedding-3-small')
        )
    else:
        return create_vector_store(_docs)

st.header("í—Œë²• Q&A ì±—ë´‡ ğŸ’¬ ğŸ“š")
option = st.selectbox("Select GPT Model", ("gpt-4o-mini", "gpt-3.5-turbo-0125"))

uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type="pdf")

if uploaded_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
        temp_file.write(uploaded_file.getvalue())
        temp_path = temp_file.name

    pages = load_and_split_pdf(temp_path)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    llm = ChatOpenAI(model=option)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ë‹¤ìŒ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”."),
            ("human", "{question}")
        ]
    )

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")
    if st.button("ì§ˆë¬¸í•˜ê¸°"):
        if question:
            with st.spinner("ìƒê° ì¤‘..."):
                result = rag_chain.invoke({"question": question})
                st.write(result)
